{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "sep_token = '<sep>'\n",
    "dataset_name = \"squad\"\n",
    "models_dir = \"saved_models/bart_base-squad\"\n",
    "checkpoint = \"facebook/bart-base\"\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "## Training\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/manuv/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50266, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.add_tokens([sep_token])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Group questions and answer with same context together\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "val_df = pd.DataFrame(dataset[\"validation\"])\n",
    "\n",
    "train_df = train_df.groupby(\"context\").agg({\"question\": list, \"answers\": list})\n",
    "val_df = val_df.groupby(\"context\").agg({\"question\": list, \"answers\": list})\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df.reset_index())\n",
    "val_dataset = datasets.Dataset.from_pandas(val_df.reset_index())\n",
    "\n",
    "dataset[\"train\"] = train_dataset\n",
    "dataset[\"validation\"] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize examples\n",
    "def convert_to_features(example_batch):\n",
    "\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['context'], \n",
    "                                                  max_length=max_input_length, \n",
    "                                                  add_special_tokens=True,\n",
    "                                                  truncation=True, \n",
    "                                                  pad_to_max_length=True)\n",
    "    \n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['question'], \n",
    "                                                   max_length=max_target_length, \n",
    "                                                   add_special_tokens=True,\n",
    "                                                   truncation=True, pad_to_max_length=True)\n",
    "                                                   \n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'], \n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'decoder_input_ids': target_encodings['input_ids'],\n",
    "        'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "def add_eos_examples(example):\n",
    "    example['context'] = example['context'] + \" \" + sep_token\n",
    "    example['question'] = \" <sep> \".join(example['question'])\n",
    "    return example\n",
    "\n",
    "\n",
    "def add_special_tokens(example):\n",
    "  example['question'] = example['question'].replace(\"{sep_token}\", sep_token)\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/18891 [00:00<?, ? examples/s]               c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "tokenized_dataset  = dataset.map(add_eos_examples)\n",
    "tokenized_dataset = tokenized_dataset.map(add_special_tokens)\n",
    "tokenized_dataset  = tokenized_dataset.map(convert_to_features,  batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did the torch arrive in Canberra? <sep> Who received the flame from Chinese officials in Canberra? <sep> What did Agnes Shea give to the Chinese in return? <sep> Who publicly argued at a press conference? <sep> Where was the relay held in Australia? <sep> How many kilometers was the route in Australia? <sep> What was the location of the beginning of the Olympic torch route in Australia? <sep> Who kept the demonstrators separated? <sep> What is the name of the Aboriginal elder who received the torch from Chinese officials?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['context', 'question', 'answers', 'input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [\"context\",\"question\",\"answers\"]\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "valid_dataset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "columns = ['input_ids', 'decoder_input_ids', 'attention_mask', 'decoder_attention_mask']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_data.pt')\n",
    "torch.save(valid_dataset, 'valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class T2TDataCollator():\n",
    "    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "        A dictionary of tensors\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        lm_labels = torch.stack([example['decoder_input_ids'] for example in batch])\n",
    "        lm_labels[lm_labels[:, :] == 0] = -100 \n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': lm_labels, \n",
    "            'decoder_attention_mask': decoder_attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(output_dir=models_dir, \n",
    "                                  per_device_train_batch_size=4, \n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  gradient_accumulation_steps=16,\n",
    "                                  learning_rate=learning_rate, \n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  logging_steps=100,\n",
    "                                  run_name=\"bart_qg_squad\",\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n"
     ]
    }
   ],
   "source": [
    "if model.device.type == 'cuda':\n",
    "    print('Model is on GPU')\n",
    "else:\n",
    "    print('Model is on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  7%|▋         | 100/1475 [04:54<1:07:42,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3547, 'learning_rate': 9.322033898305085e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  7%|▋         | 100/1475 [05:23<1:07:42,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6700265407562256, 'eval_runtime': 29.8951, 'eval_samples_per_second': 69.142, 'eval_steps_per_second': 17.294, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 200/1475 [10:15<1:01:58,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7229, 'learning_rate': 8.644067796610171e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 14%|█▎        | 200/1475 [10:45<1:01:58,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.60895574092865, 'eval_runtime': 29.9233, 'eval_samples_per_second': 69.077, 'eval_steps_per_second': 17.277, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 300/1475 [15:37<57:02,  2.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.658, 'learning_rate': 7.966101694915254e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 300/1475 [16:07<57:02,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.573846459388733, 'eval_runtime': 29.9704, 'eval_samples_per_second': 68.968, 'eval_steps_per_second': 17.25, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 400/1475 [20:58<52:18,  2.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4825, 'learning_rate': 7.288135593220338e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 27%|██▋       | 400/1475 [21:28<52:18,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5615453720092773, 'eval_runtime': 29.9529, 'eval_samples_per_second': 69.008, 'eval_steps_per_second': 17.26, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 500/1475 [26:20<47:22,  2.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4861, 'learning_rate': 6.610169491525424e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 34%|███▍      | 500/1475 [26:50<47:22,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.551958441734314, 'eval_runtime': 29.9257, 'eval_samples_per_second': 69.071, 'eval_steps_per_second': 17.276, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 600/1475 [31:47<42:30,  2.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4583, 'learning_rate': 5.932203389830509e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 41%|████      | 600/1475 [32:17<42:30,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.563263177871704, 'eval_runtime': 29.9603, 'eval_samples_per_second': 68.991, 'eval_steps_per_second': 17.256, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 700/1475 [37:09<37:24,  2.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3296, 'learning_rate': 5.254237288135594e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 47%|████▋     | 700/1475 [37:39<37:24,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5508701801300049, 'eval_runtime': 29.9364, 'eval_samples_per_second': 69.046, 'eval_steps_per_second': 17.27, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 800/1475 [42:27<32:23,  2.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3326, 'learning_rate': 4.5762711864406784e-05, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 54%|█████▍    | 800/1475 [42:57<32:23,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5446569919586182, 'eval_runtime': 29.7818, 'eval_samples_per_second': 69.405, 'eval_steps_per_second': 17.36, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 900/1475 [47:45<27:33,  2.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.312, 'learning_rate': 3.898305084745763e-05, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 61%|██████    | 900/1475 [48:15<27:33,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5582462549209595, 'eval_runtime': 29.7545, 'eval_samples_per_second': 69.468, 'eval_steps_per_second': 17.376, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1000/1475 [53:03<22:46,  2.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2359, 'learning_rate': 3.2203389830508473e-05, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 68%|██████▊   | 1000/1475 [53:32<22:46,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5484000444412231, 'eval_runtime': 29.7745, 'eval_samples_per_second': 69.422, 'eval_steps_per_second': 17.364, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1100/1475 [58:59<17:59,  2.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2267, 'learning_rate': 2.5423728813559322e-05, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▍  | 1100/1475 [59:28<17:59,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5419151782989502, 'eval_runtime': 29.7948, 'eval_samples_per_second': 69.375, 'eval_steps_per_second': 17.352, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 1200/1475 [1:04:16<13:11,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2128, 'learning_rate': 1.864406779661017e-05, 'epoch': 4.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 81%|████████▏ | 1200/1475 [1:04:46<13:11,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5518202781677246, 'eval_runtime': 29.9318, 'eval_samples_per_second': 69.057, 'eval_steps_per_second': 17.273, 'epoch': 4.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1300/1475 [1:09:34<08:23,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1624, 'learning_rate': 1.1864406779661018e-05, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 88%|████████▊ | 1300/1475 [1:10:04<08:23,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5454031229019165, 'eval_runtime': 29.7898, 'eval_samples_per_second': 69.386, 'eval_steps_per_second': 17.355, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1400/1475 [1:14:52<03:35,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.163, 'learning_rate': 5.084745762711865e-06, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 95%|█████████▍| 1400/1475 [1:15:22<03:35,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5468823909759521, 'eval_runtime': 29.7908, 'eval_samples_per_second': 69.384, 'eval_steps_per_second': 17.354, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475/1475 [1:18:58<00:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4738.3341, 'train_samples_per_second': 19.934, 'train_steps_per_second': 0.311, 'train_loss': 1.4243852311473781, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1475, training_loss=1.4243852311473781, metrics={'train_runtime': 4738.3341, 'train_samples_per_second': 19.934, 'train_steps_per_second': 0.311, 'train_loss': 1.4243852311473781, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # 80min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "trainer.save_model(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "# load the saved model\n",
    "loaded_model = BartForConditionalGeneration.from_pretrained(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input_string, **generator_args):\n",
    "    generator_args = {\n",
    "    \"max_length\": 512,\n",
    "    \"num_beams\": 4,# bij grotere num_beams is trager maar complexere vragen(niet per se betere vragen)\n",
    "    \"length_penalty\": 1.5,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    }\n",
    "    input_string = input_string + \" </s>\"\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = loaded_model.generate(input_ids, **generator_args)\n",
    "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    output = [item.split(\"<sep>\") for item in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What is the tendency to skew your choice of data sources to �those that are easily available, convenient, and/or cost-effective\"?  What happens when your featurized data doesn\\'t have a feature necessary for accurate prediction?']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Cheese is an ancient food whose origins predate recorded history. There is no conclusive evidence indicating where cheesemaking originated, whether in Europe, Central Asia or the Middle East. Earliest proposed dates for the origin of cheesemaking range from around 8000 BCE, when sheep were first domesticated. Since animal skins and inflated internal organs have, since ancient times, provided storage vessels for a range of foodstuffs, it is probable that the process of cheese making was discovered accidentally by storing milk in a container made from the stomach of an animal, resulting in the milk being turned to curd and whey by the rennet from the stomach.[7] There is a legend—with variations—about the discovery of cheese by an Arab trader who used this method of storing milk.[8]\n",
    "\n",
    "The earliest evidence of cheesemaking in the archaeological record dates back to 5500 BCE and is found in what is now Kuyavia, Poland, where strainers coated with milk-fat molecules have been found.[9]\n",
    "\n",
    "Cheesemaking may have begun independently of this by the pressing and salting of curdled milk to preserve it. Observation that the effect of making cheese in an animal stomach gave more solid and better-textured curds may have led to the deliberate addition of rennet. Early archeological evidence of Egyptian cheese has been found in Egyptian tomb murals, dating to about 2000 BCE.[10] A 2018 scientific paper stated that the world's oldest cheese, dating to approximately 1200 BCE (3200 years before present), was found in ancient Egyptian tombs.[11][12]\n",
    "\n",
    "The earliest cheeses were likely quite sour and salty, similar in texture to rustic cottage cheese or feta, a crumbly, flavorful Greek cheese. Cheese produced in Europe, where climates are cooler than the Middle East, required less salt for preservation. With less salt and acidity, the cheese became a suitable environment for useful microbes and molds, giving aged cheeses their respective flavors. The earliest ever discovered preserved cheese was found in the Taklamakan Desert in Xinjiang, China, dating back as early as 1615 BCE (3600 years before present).\n",
    "\"\"\"\n",
    "\n",
    "context_gf = \"\"\"\n",
    "#### Types of Bias\n",
    "\n",
    "**Selection bias** is the tendency to skew your choice of data sources to \n",
    "those that are easily available, convenient, and/or cost-effective. As a \n",
    "result of this a bias is introduced by the selection of individuals, groups \n",
    "or data for analysis in such a way that proper randomization is not achieved, \n",
    "thereby ensuring that the sample obtained is not representative of the \n",
    "population intended to be analyzed. \n",
    "[Learn more about selection bias](https://en.wikipedia.org/wiki/Selection_bias).\n",
    "\n",
    "**Self-selection bias** is a form of selection bias where you get the data \n",
    "from sources that “volunteered” to provide it. Most poll data has this type  \n",
    "of bias. [Learn more about self-selection bias](https://en.wikipedia.org/wiki/Self-selection_bias)\n",
    "\n",
    "**Omitted-variable bias** happens when your featurized data doesn't have a \n",
    "feature necessary for accurate prediction. For example, let's assume that you \n",
    "are working on a churn prediction model and you want to predict whether a \n",
    "customer cancels their subscription within six months. You train a model, and \n",
    "it's accurate enough; however, several weeks after deployment you see many \n",
    "unexpected false negatives. You investigate the decreased model performance \n",
    "and discover a new competitor now offers a very similar service for a lower \n",
    "price. This feature wasn't initially available to your model, therefore \n",
    "important information for accurate prediction was missing. \n",
    "[Learn more about omitted variable Bias](https://en.wikipedia.org/wiki/Omitted-variable_bias).\n",
    "\"\"\"\n",
    "\n",
    "run_model(context_gf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5_qg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
