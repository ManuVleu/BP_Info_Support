@Online{AIEngineering2021,
  author       = {AIEngineering},
  date         = {2021-03-06},
  title        = {Question Generation Using Natural Language Processing},
  url          = {https://www.youtube.com/watch?v=hoCi_bJHyb8&ab_channel=AIEngineering},
  language     = {English},
  organization = {QuestGen},
  urldate      = {2016-09-01},
  abstract     = {Ramsri Goutham is een AI expert met een specialisatie in NLP. Hij is ook de oprichter van Questgen, een bedrijf dat quizzes kan genereren op basis van een gegeven tekst. In de video bespreekt hij volgende onderwerpen: genereren van vragen met NLP, genereren van meerkeuzevragen met Wordnet en T5 transformers, genereren van waar/onwaar-vragen met GPT-2 en genereren van invulvragen en combinatievragen. Er wordt bij elk onderwerp ook een voorbeeld gegeven in Jupyter Notebook als demonstratie van de modellen besproken.},
  keywords     = {NLP, Question Generation, BERT WSD, Questgen},
}

@Article{Nema2018,
  author       = {Preksha Nema and Mitesh M. Khapra},
  date         = {2018-08-30},
  journaltitle = {EMNLP, 2018},
  title        = {Towards a Better Metric for Evaluating Question Generation Systems},
  eprint       = {1808.10192},
  eprintclass  = {cs.CL},
  eprinttype   = {arXiv},
  abstract     = {There has always been criticism for using $n$-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on $n$-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available at https://github.com/PrekshaNema25/Answerability-Metric},
  file         = {:http\://arxiv.org/pdf/1808.10192v2:PDF},
  keywords     = {cs.CL},
}

@Article{Smiderle2020,
  author       = {Rodrigo Smiderle and Sandro Jos{\'{e}} Rigo and Leonardo B. Marques and Jorge Arthur Pe{\c{c}}anha de Miranda Coelho and Patricia A. Jaques},
  date         = {2020-01},
  journaltitle = {Smart Learning Environments},
  title        = {The impact of gamification on students' learning, engagement and behavior based on their personality traits},
  doi          = {10.1186/s40561-019-0098-x},
  number       = {1},
  volume       = {7},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Comment{jabref-meta: databaseType:biblatex;}
