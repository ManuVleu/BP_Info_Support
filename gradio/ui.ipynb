{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface with Gradio for QG & QA\n",
    "T5 Answer Agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, BartForConditionalGeneration, AutoTokenizer\n",
    "import Levenshtein\n",
    "import os\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from utils import Chapter, File\n",
    "\n",
    "# SAVE_TREE_DIR = os.path.join(\"C:\", os.sep, \"Users\", \"ManuV\", \"Documents\", \"Bachelorproef\", \"BP_Info_Support\", \"data\")\n",
    "# SAVE_TREE_DIR = os.path.join(\"E:\", os.sep, \"HOGENT\", \"2022_2023\", \"BA\",\"BP_Info_Support\",\"data\")\n",
    "# C:\\Users\\dama_\\OneDrive\\Desktop\\HoGent\\2022_2023\\BP\\BP_Info_Support\n",
    "SAVE_TREE_DIR = os.path.join(\"C:\", os.sep, \"Users\", \"dama_\", \"OneDrive\",\"Desktop\",\"HoGent\",\"2022_2023\",\"BP\",\"BP_Info_Support\",\"data\")\n",
    "\n",
    "QG_MODEL_NAME = \"ManuVleuBeu/T5_ag_SQuAD\"\n",
    "QA_MODEL_NAME = \"damapika/roberta-base_mod_squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guidance_framework():\n",
    "    with open(os.path.join(SAVE_TREE_DIR,\"gf_structure.pkl\"), \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "root = get_guidance_framework()\n",
    "    \n",
    "def get_all_files():\n",
    "    chapters = root.get_all_chapters()\n",
    "\n",
    "    all_files = []\n",
    "    for chapter in chapters:\n",
    "        all_files.extend(chapter.files)\n",
    "\n",
    "    return all_files\n",
    "\n",
    "files = get_all_files()\n",
    "\n",
    "def clean_file_dirs():\n",
    "    for file in files:\n",
    "        file_dir = '/'.join(file.dir.split('docs\\\\')[1:])\n",
    "        file.dir = file_dir.replace('\\\\','/')\n",
    "\n",
    "    return files\n",
    "\n",
    "files = clean_file_dirs()\n",
    "\n",
    "def get_contexts_from_file_dir(file_dir):\n",
    "    file = [file for file in files if file.dir == file_dir]\n",
    "    if not len(file) == 0:\n",
    "        file = file[0]\n",
    "        # Sorts text per h-elem by biggest\n",
    "        key_ranking = sorted(file.text.keys(), key=lambda k: len(' '.join(file.text[k])), reverse=True)\n",
    "        return [' '.join(file.text[key]) for key in key_ranking]\n",
    "    else:\n",
    "        raise Exception(\"File directory does not exist!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\inputs.py:219: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\inputs.py:222: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\dama_\\AppData\\Local\\Temp\\ipykernel_14204\\4064746390.py:54: UserWarning: You have unused kwarg parameters in Interface, please remove them: {'interface_id': 'io2'}\n",
      "  io2 = gr.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_all_files()\n",
    "\n",
    "# Question Generation Model\n",
    "question_generator = pipeline(\"text2text-generation\", model=QG_MODEL_NAME)\n",
    "# question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qa-qg-hl\")\n",
    "\n",
    "# Answer Generation Model\n",
    "answer_generator = pipeline(\"question-answering\", model=QA_MODEL_NAME)\n",
    "# TODO: Add all chapter contexts or an extra function where user selects chapter->context\n",
    "contexts = [file.dir for file in files]\n",
    "selected_context = []\n",
    "generated_question = []\n",
    "\n",
    "def calculate_similarity(answer1, answer2):\n",
    "    similarity_score = 1 - (Levenshtein.distance(answer1.lower(), answer2.lower()) / max(len(answer1), len(answer2)))\n",
    "    return similarity_score\n",
    "\n",
    "def generate_question(file_dir):\n",
    "    global selected_context\n",
    "    global generated_question\n",
    "    contexts = get_contexts_from_file_dir(file_dir)\n",
    "    selected_context.clear()\n",
    "    generated_question.clear()\n",
    "    for context in contexts:\n",
    "        question = question_generator(context)[0]['generated_text']\n",
    "        question = question.split('?')[0]+'?'# Selects first question\n",
    "        # questions = [question.strip() for question in string.split('?') if question.endswith('?')] instead of first question takes multiple per context\n",
    "        generated_question.append(question)\n",
    "    selected_context = contexts\n",
    "    return '\\n'.join(generated_question)\n",
    "\n",
    "# Interface for QG\n",
    "io1 = gr.Interface(\n",
    "    fn=generate_question,\n",
    "    inputs=[\n",
    "        gr.inputs.Dropdown(choices=contexts, label=\"Select a context\", type=\"value\"),\n",
    "    ],\n",
    "    live=False,\n",
    "    outputs=[\n",
    "        gr.outputs.Textbox(label=\"Generated Question\"),\n",
    "    ],\n",
    "    title=\"Quiz Generator\",\n",
    "    description=\"Select a context, generate a question, and compare your answer.\",\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "def generate_answer(question,user_answer):\n",
    "    context = selected_context[generated_question.index(question)]\n",
    "    answer = answer_generator(context=context,question=question)\n",
    "    score=calculate_similarity(user_answer,answer['answer'])\n",
    "    return [user_answer,answer['answer'],score]\n",
    "io1.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional interface for answer checking\n",
    "io2 = gr.Interface(\n",
    "    fn=generate_answer,\n",
    "    live=False,\n",
    "    inputs=[gr.inputs.Dropdown(choices=generated_question,label=\"Select a question\",type=\"value\"), gr.outputs.Textbox(label=\"User Answer\")],\n",
    "    outputs=[gr.outputs.Textbox(label=\"User Answer\"),gr.outputs.Textbox(label=\"Generated Answer\"),gr.outputs.Textbox(label=\"Levenshtein Answer score\")],\n",
    "    title=\"Answer Checker\",\n",
    "    description=\"Compare your answer.\",\n",
    "    allow_flagging=\"never\",\n",
    "    interface_id = \"io2\"\n",
    ")\n",
    "\n",
    "io2.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART_ans_SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, BartForConditionalGeneration, AutoTokenizer\n",
    "import Levenshtein\n",
    "import os\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from utils import Chapter, File\n",
    "import spacy\n",
    "import textrazor\n",
    "import random\n",
    "\n",
    "SAVE_TREE_DIR = os.path.join(\"C:\", os.sep, \"Users\", \"ManuV\", \"Documents\", \"Bachelorproef\", \"BP_Info_Support\", \"data\")\n",
    "QG_MODEL_NAME = \"ManuVleuBeu/BART_ans_SQuAD\"\n",
    "QA_MODEL_NAME = \"damapika/roberta-base_mod_squad\"\n",
    "TEXTRAZOR_APIKEY = \"42acc4018111e29f626f458272bd34981a5b2b268c5fc8dbcacd7ab5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guidance_framework():\n",
    "    with open(os.path.join(SAVE_TREE_DIR,\"gf_structure.pkl\"), \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "root = get_guidance_framework()\n",
    "    \n",
    "def get_all_files():\n",
    "    chapters = root.get_all_chapters()\n",
    "\n",
    "    all_files = []\n",
    "    for chapter in chapters:\n",
    "        all_files.extend(chapter.files)\n",
    "\n",
    "    return all_files\n",
    "\n",
    "files = get_all_files()\n",
    "\n",
    "def clean_file_dirs():\n",
    "    for file in files:\n",
    "        file_dir = '/'.join(file.dir.split('docs\\\\')[1:])\n",
    "        file.dir = file_dir.replace('\\\\','/')\n",
    "\n",
    "    return files\n",
    "\n",
    "files = clean_file_dirs()\n",
    "\n",
    "def get_contexts_from_file_dir(file_dir):\n",
    "    file = [file for file in files if file.dir == file_dir]\n",
    "    if not len(file) == 0:\n",
    "        file = file[0]\n",
    "        # Sorts text per h-elem by biggest\n",
    "        key_ranking = sorted(file.text.keys(), key=lambda k: len(' '.join(file.text[k])), reverse=True)\n",
    "        return [' '.join(file.text[key]) for key in key_ranking]\n",
    "    else:\n",
    "        raise Exception(\"File directory does not exist!\")\n",
    "    \n",
    "def run_model(input_string,target_answer,tokenizer,model, **generator_args):\n",
    "    generator_args = {\n",
    "    \"max_length\": 768,\n",
    "    \"num_beams\": 4,# bij grotere num_beams is trager maar complexere vragen(niet per se betere vragen)\n",
    "    \"length_penalty\": 1.5,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    }\n",
    "    \n",
    "    input_string = input_string + \" \" + \"<ANSWER>\" + target_answer + \" </s>\"\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = model.generate(input_ids.to(\"cuda:0\"), **generator_args)\n",
    "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    output = [item.split(\"<sep>\") for item in output]\n",
    "    return output[0][0]\n",
    "\n",
    "def get_marked_answers(context):\n",
    "    marked_answers = []\n",
    "\n",
    "    # TextRank\n",
    "    # If error -> python -m spacy download en_core_web_sm\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # nlp.add_pipe(\"textrank\")\n",
    "    # doc = nlp(context)\n",
    "\n",
    "    # for phrase in doc._.phrases[:2]:\n",
    "    #     marked_answers.append(phrase.text.replace('\\n',''))\n",
    "\n",
    "    # TextRazor\n",
    "    textrazor.api_key = TEXTRAZOR_APIKEY\n",
    "    client = textrazor.TextRazor(extractors=[\"entities\",\"topics\"])\n",
    "    response = client.analyze(context)\n",
    "\n",
    "    textrazor_results = {\"word\": [], \"score\": []}\n",
    "    for entity in response.entities():\n",
    "        textrazor_results[\"score\"].append(entity.relevance_score)\n",
    "        textrazor_results[\"word\"].append(entity.id)\n",
    "\n",
    "    # Sort the words based on scores in descending order\n",
    "    sorted_words = sorted(textrazor_results[\"word\"], key=lambda w: textrazor_results[\"score\"][textrazor_results[\"word\"].index(w)], reverse=True)\n",
    "\n",
    "    # Get the top 3 words\n",
    "    top_words = sorted_words[:3]\n",
    "\n",
    "    marked_answers.extend(top_words)\n",
    "\n",
    "    marked_answers = list(set(marked_answers))\n",
    "    # random.shuffle(marked_answers)\n",
    "\n",
    "    return marked_answers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\gradio\\inputs.py:219: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\gradio\\inputs.py:222: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "files = get_all_files()\n",
    "\n",
    "# Question Generation Model\n",
    "qg_model = BartForConditionalGeneration.from_pretrained(QG_MODEL_NAME).to(\"cuda:0\")\n",
    "qg_tokenizer = AutoTokenizer.from_pretrained(QG_MODEL_NAME)\n",
    "\n",
    "# Answer Generation Model\n",
    "answer_generator = pipeline(\"question-answering\", model=QA_MODEL_NAME)\n",
    "# TODO: Add all chapter contexts or an extra function where user selects chapter->context\n",
    "contexts = [file.dir for file in files]\n",
    "selected_context = []\n",
    "generated_question = []\n",
    "selected_answers = []\n",
    "\n",
    "def calculate_similarity(answer1, answer2):\n",
    "    similarity_score = 1 - (Levenshtein.distance(answer1.lower(), answer2.lower()) / max(len(answer1), len(answer2)))\n",
    "    return similarity_score\n",
    "\n",
    "def generate_question(file_dir):\n",
    "    global selected_context\n",
    "    global generated_question\n",
    "    global selected_answers\n",
    "    contexts = get_contexts_from_file_dir(file_dir)\n",
    "    selected_context.clear()\n",
    "    generated_question.clear()\n",
    "    \n",
    "    for context in contexts:\n",
    "        answer = get_marked_answers(context)[0]\n",
    "        selected_answers.append(answer)\n",
    "        question = run_model(context,answer,qg_tokenizer,qg_model)\n",
    "        question = question.split('?')[0]+'?'# Selects first question\n",
    "        # questions = [question.strip() for question in string.split('?') if question.endswith('?')] instead of first question takes multiple per context\n",
    "        generated_question.append(question)\n",
    "    selected_context = contexts\n",
    "    return '\\n'.join(generated_question)\n",
    "\n",
    "# Interface for QG\n",
    "io1 = gr.Interface(\n",
    "    fn=generate_question,\n",
    "    inputs=[\n",
    "        gr.inputs.Dropdown(choices=contexts, label=\"Select a context\", type=\"value\"),\n",
    "    ],\n",
    "    live=False,\n",
    "    outputs=[\n",
    "        gr.outputs.Textbox(label=\"Generated Question\"),\n",
    "    ],\n",
    "    title=\"Quiz Generator\",\n",
    "    description=\"Select a context, generate a question, and compare your answer.\",\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io1.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\gradio\\inputs.py:219: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\gradio\\inputs.py:222: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\quiz_generator\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\ManuV\\AppData\\Local\\Temp\\ipykernel_12172\\1161264703.py:8: UserWarning: You have unused kwarg parameters in Interface, please remove them: {'interface_id': 'io2'}\n",
      "  io2 = gr.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer(question,user_answer):\n",
    "     #context = selected_context[generated_question.index(question)]\n",
    "    answer = selected_answers[generated_question.index(question)]\n",
    "    score=calculate_similarity(user_answer,answer)\n",
    "    return [user_answer,answer,score]\n",
    "\n",
    "# Additional interface for answer checking\n",
    "io2 = gr.Interface(\n",
    "    fn=generate_answer,\n",
    "    live=False,\n",
    "    inputs=[gr.inputs.Dropdown(choices=generated_question,label=\"Select a question\",type=\"value\"), gr.outputs.Textbox(label=\"User Answer\")],\n",
    "    outputs=[gr.outputs.Textbox(label=\"User Answer\"),gr.outputs.Textbox(label=\"Generated Answer\"),gr.outputs.Textbox(label=\"Levenshtein Answer score\")],\n",
    "    title=\"Answer Checker\",\n",
    "    description=\"Compare your answer.\",\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "io2.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
