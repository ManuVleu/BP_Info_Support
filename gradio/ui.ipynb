{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline,BartForConditionalGeneration, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = BartForConditionalGeneration.from_pretrained(\"ManuVleuBeu/BART_answer-aware_MSMARCO\").to(\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ManuVleuBeu/BART_answer-aware_MSMARCO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input_string,target_answer, **generator_args):\n",
    "    generator_args = {\n",
    "    \"max_length\": 768,\n",
    "    \"num_beams\": 4,# bij grotere num_beams is trager maar complexere vragen(niet per se betere vragen)\n",
    "    \"length_penalty\": 1.5,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    }\n",
    "    \n",
    "    input_string = input_string + \" \" + \"<ANSWER>\" + target_answer + \" </s>\"\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = loaded_model.generate(input_ids.to(\"cuda:0\"), **generator_args)\n",
    "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    output = [item.split(\"<sep>\") for item in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who is Fyodor Dostoevsky?\"\n",
    "#TODO: add a select chapter from GF context\n",
    "#TODO: get context \n",
    "context = \"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\"\n",
    "\n",
    "pipe_qg=pipeline(\"text2text-generation\",model=\"ManuVleuBeu/BART_answer-aware_MSMARCO\")\n",
    "pipe_qa = pipeline(\"question-answering\", model=\"damapika/roberta-base_mod\")\n",
    "\n",
    "def getQuestion(context):\n",
    "  print(pipe_qg(context=context))\n",
    "  \n",
    "def getAnswer(question, context):\n",
    "  return pipe_qa(question=question, context=context)['answer']\n",
    "\n",
    "getQuestion(context)\n",
    "\n",
    "demo = gr.Interface(\n",
    "  fn=getAnswer, \n",
    "  inputs=['text','text'],\n",
    "  outputs='text',\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    question = gr.Textbox(label=\"Question\")\n",
    "    answer = gr.Textbox(label=\"Answer\")\n",
    "    ans_btn = gr.Button(\"Check answer\")\n",
    "    ans_btn.click(fn=greet, inputs=question, outputs=answer, api_name=\"greet\")\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from utils import Chapter, File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GF_FOLDER = os.path.join(\"C:\", os.sep, \"Users\", \"manuv\", \"OneDrive\", \"Documenten\", \"Code\", \"BAP\", \"BP_gitrepo\", \"BP_Info_Support\", \"docs\")\n",
    "STRUCTURE_FILE = \"structure.md\"\n",
    "CHAPTER_TREE = Chapter(\"root\",GF_FOLDER)\n",
    "\n",
    "SAVE_TREE_DIR = os.path.join(\"C:\", os.sep, \"Users\", \"manuv\", \"OneDrive\", \"Documenten\", \"Code\", \"BAP\", \"BP_gitrepo\", \"BP_Info_Support\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GF_FOLDER = os.path.join(\"E:\", os.sep, \"HOGENT\", \"2022_2023\", \"BA\",\"docs\")\n",
    "STRUCTURE_FILE = \"structure.md\"\n",
    "CHAPTER_TREE = Chapter(\"root\",GF_FOLDER)\n",
    "SAVE_TREE_DIR = os.path.join(\"E:\", os.sep, \"HOGENT\", \"2022_2023\", \"BA\",\"docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guidance_framework():\n",
    "    with open(os.path.join(SAVE_TREE_DIR,\"gf_structure.pkl\"), \"rb\") as f:\n",
    "        root = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chapter_by_dir(root, dir):\n",
    "    if root.dir == dir:\n",
    "        return root\n",
    "    for child in root.children:\n",
    "        found_chapter = find_chapter_by_dir(child, dir)\n",
    "        if found_chapter:\n",
    "            return found_chapter\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_chapter = CHAPTER_TREE.get_random_chapter()\n",
    "print(f'Chapter Name: {rnd_chapter.name}')\n",
    "rnd_file = rnd_chapter.get_random_file()\n",
    "print(f'File Name: {rnd_file.name}')\n",
    "print(f'File Dir: {rnd_file.dir}')\n",
    "print('Structured Text in file')\n",
    "rnd_file.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# Question Generation Model\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qa-qg-hl\")\n",
    "\n",
    "# Answer Comparison Model\n",
    "answer_comparison = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def generate_question(context, question_generator):\n",
    "    question = question_generator(context)[0]['question']\n",
    "    return question\n",
    "\n",
    "def check_answer(context, question, user_answer, answer_comparison):\n",
    "    answer = answer_comparison({\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    })['answer']\n",
    "    return answer\n",
    "\n",
    "contexts = [\"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\", \"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\", \"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\"]  # Add your contexts to this list\n",
    "\n",
    "def get_context(context):\n",
    "    return context\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_question,\n",
    "    inputs=[gr.inputs.Dropdown(choices=contexts, label=\"Select a context\")],\n",
    "    outputs=[\"text\"],\n",
    "    title=\"Quiz Generator\",\n",
    "    description=\"Generate a question from a selected context.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\inputs.py:219: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\inputs.py:222: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  super().__init__(\n",
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:760: UserWarning: Expected 0 arguments for function <function generate_answer at 0x0000017EBC63D1C0>, received 1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:768: UserWarning: Expected maximum 0 arguments for function <function generate_answer at 0x0000017EBC63D1C0>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\routes.py\", line 414, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1323, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\blocks.py\", line 1051, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: generate_answer() takes 0 positional arguments but 1 was given\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "from gradio import Dropdown\n",
    "import Levenshtein\n",
    "\n",
    "# Question Generation Model\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"ManuVleuBeu/T5_ag_SQuAD\")\n",
    "\n",
    "# Answer Generation Model\n",
    "answer_generator = pipeline(\"question-answering\", model=\"damapika/roberta-base_mod_squad\")\n",
    "# TODO: Add all chapter contexts or an extra function where user selects chapter->context\n",
    "contexts = [\n",
    "    \"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\", \n",
    "    \"My name is Sarah and I live in London.\"\n",
    "]  # Add your contexts to this list\n",
    "selected_context=[None]\n",
    "generated_question=[None]\n",
    "generated_answer=[None]\n",
    "\n",
    "def calculate_similarity(answer1, answer2):\n",
    "    similarity_score = 1 - (Levenshtein.distance(answer1.lower(), answer2.lower()) / max(len(answer1), len(answer2)))\n",
    "    return similarity_score\n",
    "\n",
    "def generate_question(context):\n",
    "    selected_context[0] = context\n",
    "    question = question_generator(context)[0]['generated_text']\n",
    "    question = question.split('?')[0]+'?'\n",
    "    generated_question[0]=question\n",
    "    return question\n",
    "\n",
    "def generate_answer(user_answer):\n",
    "    question = generated_question[0]\n",
    "    context = selected_context[0]\n",
    "    answer = answer_generator(context=context,question=question)\n",
    "    print(answer['answer'])\n",
    "    generated_answer[0] = answer['answer']\n",
    "    score=calculate_similarity(user_answer,generated_answer[0])\n",
    "    return [user_answer,answer['answer'],score]\n",
    "\n",
    "io1 = gr.Interface(\n",
    "    fn=generate_question,\n",
    "    inputs=[\n",
    "        gr.inputs.Dropdown(choices=contexts, label=\"Select a context\", type=\"value\"),\n",
    "    ],\n",
    "    live=False,\n",
    "    outputs=[\n",
    "        gr.outputs.Textbox(label=\"Generated Question\")\n",
    "    ],\n",
    "    title=\"Quiz Generator\",\n",
    "    description=\"Select a context, generate a question, and compare your answer.\",\n",
    "    allow_flagging=\"never\"\n",
    "\n",
    ")\n",
    "\n",
    "# Additional interface for answer checking\n",
    "io2 = gr.Interface(\n",
    "    fn=generate_answer,\n",
    "    live=False,\n",
    "    inputs=[gr.outputs.Textbox(label=\"User Answer\")],\n",
    "    outputs=[gr.outputs.Textbox(label=\"User Answer\"),gr.outputs.Textbox(label=\"Generated Answer\"),gr.outputs.Textbox(label=\"Levenshtein Answer score\")],\n",
    "    title=\"Answer Checker\",\n",
    "    description=\"Compare your answer.\"\n",
    ")\n",
    "\n",
    "io1.launch()\n",
    "io2.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
