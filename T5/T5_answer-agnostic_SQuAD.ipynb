{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "```\n",
    "pip install transformers\n",
    "pip install datasets\n",
    "pip install nltk\n",
    "```\n",
    "Installeer [CUDA](https://developer.nvidia.com/cuda-downloads) en [PyTorch](https://pytorch.org/get-started/locally/) als je op GPU wilt trainen anders installeer alleen PyTorch op CPU\n",
    "\n",
    "## Source\n",
    "[source](https://colab.research.google.com/drive/1z-Zl2hftMrFXabYfmz8o9YZpgYx6sGeW?usp=sharing#scrollTo=OG82dfGbK63x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "sep_token = '<sep>'\n",
    "dataset_name = \"wiselinjayajos/squad_modified_for_t5_qg\"\n",
    "models_dir = \"saved_models/t5_base-squad_mod\"\n",
    "checkpoint = 't5-base'\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "## Training\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/manuv/.cache/huggingface/datasets/wiselinjayajos___parquet/wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 20.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'generate questions: Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'questions': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? {sep_token} What is in front of the Notre Dame Main Building? {sep_token} The Basilica of the Sacred heart at Notre Dame is beside to which structure? {sep_token} What is the Grotto at Notre Dame? {sep_token} What sits on top of the Main Building at Notre Dame? {sep_token}'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = transformers.T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "tokenizer = transformers.T5TokenizerFast.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32102, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.add_tokens([sep_token])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize examples\n",
    "def convert_to_features(example_batch):\n",
    "\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['context'], \n",
    "                                                  max_length=max_input_length, \n",
    "                                                  add_special_tokens=True,\n",
    "                                                  truncation=True, \n",
    "                                                  pad_to_max_length=True)\n",
    "    \n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['questions'], \n",
    "                                                   max_length=max_target_length, \n",
    "                                                   add_special_tokens=True,\n",
    "                                                   truncation=True, pad_to_max_length=True)\n",
    "                                                   \n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'], \n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'decoder_input_ids': target_encodings['input_ids'],\n",
    "        'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings\n",
    "\n",
    "def add_eos_examples(example):\n",
    "    example['context'] = example['context'] + \" \" + sep_token\n",
    "    example['questions'] = example['questions'] + \" \" + sep_token\n",
    "    return example\n",
    "\n",
    "\n",
    "def add_special_tokens(example):\n",
    "  example['questions'] = example['questions'].replace(\"{sep_token}\", sep_token)\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\manuv\\.cache\\huggingface\\datasets\\wiselinjayajos___parquet\\wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-86b2ba036c9ec29b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\manuv\\.cache\\huggingface\\datasets\\wiselinjayajos___parquet\\wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-aeff52f71385625f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\manuv\\.cache\\huggingface\\datasets\\wiselinjayajos___parquet\\wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-65b56a4d05dede59.arrow\n",
      "Loading cached processed dataset at C:\\Users\\manuv\\.cache\\huggingface\\datasets\\wiselinjayajos___parquet\\wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-3493074a568b364e.arrow\n",
      "Map:   0%|          | 0/18896 [00:00<?, ? examples/s]c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   5%|▌         | 1000/18896 [00:00<00:05, 3256.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  11%|█         | 2000/18896 [00:00<00:04, 3640.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  16%|█▌        | 3000/18896 [00:00<00:04, 3876.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  21%|██        | 4000/18896 [00:01<00:03, 3985.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  26%|██▋       | 5000/18896 [00:01<00:03, 4013.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  32%|███▏      | 6000/18896 [00:01<00:03, 3960.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  37%|███▋      | 7000/18896 [00:01<00:03, 3852.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  42%|████▏     | 8000/18896 [00:02<00:02, 3771.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  48%|████▊     | 9000/18896 [00:02<00:02, 3693.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  53%|█████▎    | 10000/18896 [00:02<00:02, 3209.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  58%|█████▊    | 11000/18896 [00:03<00:02, 3380.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  64%|██████▎   | 12000/18896 [00:03<00:01, 3488.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  69%|██████▉   | 13000/18896 [00:03<00:01, 3613.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  74%|███████▍  | 14000/18896 [00:03<00:01, 3668.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  79%|███████▉  | 15000/18896 [00:04<00:01, 3711.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  85%|████████▍ | 16000/18896 [00:04<00:00, 3734.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  90%|████████▉ | 17000/18896 [00:04<00:00, 3762.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  95%|█████████▌| 18000/18896 [00:04<00:00, 3791.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 896\n",
      "<class 'list'> 896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2067 [00:00<?, ? examples/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  48%|████▊     | 1000/2067 [00:00<00:00, 3890.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1000\n",
      "<class 'list'> 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 67\n",
      "<class 'list'> 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "tokenized_dataset  = dataset.map(add_eos_examples)\n",
    "tokenized_dataset = tokenized_dataset.map(add_special_tokens)\n",
    "tokenized_dataset  = tokenized_dataset.map(convert_to_features,  batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? <sep> What is in front of the Notre Dame Main Building? <sep> The Basilica of the Sacred heart at Notre Dame is beside to which structure? <sep> What is the Grotto at Notre Dame? <sep> What sits on top of the Main Building at Notre Dame? <sep> <sep>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [\"context\", \"questions\"]\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "valid_dataset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "columns = ['input_ids', 'decoder_input_ids', 'attention_mask', 'decoder_attention_mask']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_data.pt')\n",
    "torch.save(valid_dataset, 'valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class T2TDataCollator():\n",
    "    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "        A dictionary of tensors\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        lm_labels = torch.stack([example['decoder_input_ids'] for example in batch])\n",
    "        lm_labels[lm_labels[:, :] == 0] = -100 \n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': lm_labels, \n",
    "            'decoder_attention_mask': decoder_attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(output_dir=models_dir, \n",
    "                                  per_device_train_batch_size=4, \n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  gradient_accumulation_steps=16,\n",
    "                                  learning_rate=learning_rate, \n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  logging_steps=100,\n",
    "                                  run_name=\"t5_qg_squad\",\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  save_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n"
     ]
    }
   ],
   "source": [
    "if model.device.type == 'cuda':\n",
    "    print('Model is on GPU')\n",
    "else:\n",
    "    print('Model is on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  5%|▍         | 100/2065 [09:52<3:13:04,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6627, 'learning_rate': 9.515738498789348e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  5%|▍         | 100/2065 [10:54<3:13:04,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.982875943183899, 'eval_runtime': 62.9394, 'eval_samples_per_second': 32.841, 'eval_steps_per_second': 8.214, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 200/2065 [20:47<3:03:30,  5.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0325, 'learning_rate': 9.031476997578692e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 10%|▉         | 200/2065 [21:50<3:03:30,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7665561437606812, 'eval_runtime': 63.0233, 'eval_samples_per_second': 32.797, 'eval_steps_per_second': 8.203, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 300/2065 [31:40<2:53:36,  5.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9027, 'learning_rate': 8.54721549636804e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 15%|█▍        | 300/2065 [32:43<2:53:36,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7088032960891724, 'eval_runtime': 63.0209, 'eval_samples_per_second': 32.799, 'eval_steps_per_second': 8.204, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 400/2065 [42:33<2:43:30,  5.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7977, 'learning_rate': 8.062953995157385e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 19%|█▉        | 400/2065 [43:36<2:43:30,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6800835132598877, 'eval_runtime': 62.9674, 'eval_samples_per_second': 32.827, 'eval_steps_per_second': 8.211, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 500/2065 [53:24<2:30:42,  5.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7658, 'learning_rate': 7.578692493946733e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 24%|██▍       | 500/2065 [54:25<2:30:42,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.657753586769104, 'eval_runtime': 61.679, 'eval_samples_per_second': 33.512, 'eval_steps_per_second': 8.382, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 600/2065 [1:04:07<2:20:04,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7384, 'learning_rate': 7.094430992736077e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 29%|██▉       | 600/2065 [1:05:08<2:20:04,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6324788331985474, 'eval_runtime': 61.2964, 'eval_samples_per_second': 33.721, 'eval_steps_per_second': 8.434, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 700/2065 [1:14:42<2:10:34,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6815, 'learning_rate': 6.610169491525424e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 34%|███▍      | 700/2065 [1:15:44<2:10:34,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.630340576171875, 'eval_runtime': 61.3513, 'eval_samples_per_second': 33.691, 'eval_steps_per_second': 8.427, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 800/2065 [1:25:18<2:00:44,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6762, 'learning_rate': 6.12590799031477e-05, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 39%|███▊      | 800/2065 [1:26:19<2:00:44,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6148935556411743, 'eval_runtime': 61.2034, 'eval_samples_per_second': 33.773, 'eval_steps_per_second': 8.447, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 900/2065 [1:35:51<1:51:08,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6586, 'learning_rate': 5.641646489104117e-05, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 44%|████▎     | 900/2065 [1:36:53<1:51:08,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6111118793487549, 'eval_runtime': 61.1889, 'eval_samples_per_second': 33.781, 'eval_steps_per_second': 8.449, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1000/2065 [1:46:30<1:42:19,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.617, 'learning_rate': 5.157384987893463e-05, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 48%|████▊     | 1000/2065 [1:47:31<1:42:19,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.606593132019043, 'eval_runtime': 61.5256, 'eval_samples_per_second': 33.596, 'eval_steps_per_second': 8.403, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1100/2065 [1:57:58<1:32:42,  5.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6137, 'learning_rate': 4.673123486682809e-05, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 53%|█████▎    | 1100/2065 [1:58:59<1:32:42,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5923593044281006, 'eval_runtime': 61.566, 'eval_samples_per_second': 33.574, 'eval_steps_per_second': 8.397, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1200/2065 [2:08:37<1:23:07,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5948, 'learning_rate': 4.188861985472155e-05, 'epoch': 4.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 58%|█████▊    | 1200/2065 [2:09:38<1:23:07,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6054214239120483, 'eval_runtime': 61.5201, 'eval_samples_per_second': 33.599, 'eval_steps_per_second': 8.404, 'epoch': 4.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1300/2065 [2:19:15<1:13:36,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5786, 'learning_rate': 3.7046004842615015e-05, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 63%|██████▎   | 1300/2065 [2:20:16<1:13:36,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5960665941238403, 'eval_runtime': 61.5627, 'eval_samples_per_second': 33.576, 'eval_steps_per_second': 8.398, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1400/2065 [2:29:50<1:03:37,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5673, 'learning_rate': 3.2203389830508473e-05, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 68%|██████▊   | 1400/2065 [2:30:53<1:03:37,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.586434245109558, 'eval_runtime': 62.797, 'eval_samples_per_second': 32.916, 'eval_steps_per_second': 8.233, 'epoch': 4.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1500/2065 [2:40:26<53:59,  5.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5568, 'learning_rate': 2.7360774818401942e-05, 'epoch': 5.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 73%|███████▎  | 1500/2065 [2:41:28<53:59,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.589478850364685, 'eval_runtime': 61.2575, 'eval_samples_per_second': 33.743, 'eval_steps_per_second': 8.44, 'epoch': 5.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1600/2065 [2:51:07<44:21,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5383, 'learning_rate': 2.25181598062954e-05, 'epoch': 5.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 77%|███████▋  | 1600/2065 [2:52:09<44:21,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5902950763702393, 'eval_runtime': 61.1955, 'eval_samples_per_second': 33.777, 'eval_steps_per_second': 8.448, 'epoch': 5.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1700/2065 [3:01:42<34:48,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5261, 'learning_rate': 1.7675544794188863e-05, 'epoch': 5.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 82%|████████▏ | 1700/2065 [3:02:43<34:48,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.586995005607605, 'eval_runtime': 61.157, 'eval_samples_per_second': 33.798, 'eval_steps_per_second': 8.454, 'epoch': 5.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1800/2065 [3:12:16<25:16,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5406, 'learning_rate': 1.2832929782082325e-05, 'epoch': 6.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 87%|████████▋ | 1800/2065 [3:13:17<25:16,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5864695310592651, 'eval_runtime': 61.1648, 'eval_samples_per_second': 33.794, 'eval_steps_per_second': 8.453, 'epoch': 6.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1900/2065 [3:22:49<15:44,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5185, 'learning_rate': 7.990314769975787e-06, 'epoch': 6.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 92%|█████████▏| 1900/2065 [3:23:50<15:44,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5874444246292114, 'eval_runtime': 61.1579, 'eval_samples_per_second': 33.798, 'eval_steps_per_second': 8.454, 'epoch': 6.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2000/2065 [3:33:23<06:11,  5.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.517, 'learning_rate': 3.1476997578692496e-06, 'epoch': 6.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 97%|█████████▋| 2000/2065 [3:34:24<06:11,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5853705406188965, 'eval_runtime': 61.133, 'eval_samples_per_second': 33.812, 'eval_steps_per_second': 8.457, 'epoch': 6.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2065/2065 [3:40:42<00:00,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13242.2891, 'train_samples_per_second': 9.989, 'train_steps_per_second': 0.156, 'train_loss': 1.6986265316425166, 'epoch': 6.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2065, training_loss=1.6986265316425166, metrics={'train_runtime': 13242.2891, 'train_samples_per_second': 9.989, 'train_steps_per_second': 0.156, 'train_loss': 1.6986265316425166, 'epoch': 6.99})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training https://colab.research.google.com/drive/1z-Zl2hftMrFXabYfmz8o9YZpgYx6sGeW?usp=sharing#scrollTo=OG82dfGbK63x\n",
    "trainer.train()#3u45-4u op RTX 3060 12GB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "trainer.save_model(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "# load the saved model\n",
    "loaded_model = T5ForConditionalGeneration.from_pretrained('saved_models/t5_base-squad_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manuv\\miniconda3\\envs\\t5_qg\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"saved_models/t5_base-squad_mod\"\n",
    "checkpoint = 't5-base'\n",
    "tokenizer = T5TokenizerFast.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(input_string, **generator_args):\n",
    "    generator_args = {\n",
    "    \"max_length\": 512,\n",
    "    \"num_beams\": 4,# bij grotere num_beams is trager maar complexere vragen(niet per se betere vragen)\n",
    "    \"length_penalty\": 1.5,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"early_stopping\": True,\n",
    "    }\n",
    "    input_string = \"generate questions: \" + input_string + \" </s>\"\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = loaded_model.generate(input_ids, **generator_args)\n",
    "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    output = [item.split(\"<sep>\") for item in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"What is an ancient food whose origins predate recorded history? When were sheep first domesticated? Where is the earliest evidence of cheesemaking found? In what year was the world's oldest cheese discovered?\"]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Cheese is an ancient food whose origins predate recorded history. There is no conclusive evidence indicating where cheesemaking originated, whether in Europe, Central Asia or the Middle East. Earliest proposed dates for the origin of cheesemaking range from around 8000 BCE, when sheep were first domesticated. Since animal skins and inflated internal organs have, since ancient times, provided storage vessels for a range of foodstuffs, it is probable that the process of cheese making was discovered accidentally by storing milk in a container made from the stomach of an animal, resulting in the milk being turned to curd and whey by the rennet from the stomach.[7] There is a legend—with variations—about the discovery of cheese by an Arab trader who used this method of storing milk.[8]\n",
    "\n",
    "The earliest evidence of cheesemaking in the archaeological record dates back to 5500 BCE and is found in what is now Kuyavia, Poland, where strainers coated with milk-fat molecules have been found.[9]\n",
    "\n",
    "Cheesemaking may have begun independently of this by the pressing and salting of curdled milk to preserve it. Observation that the effect of making cheese in an animal stomach gave more solid and better-textured curds may have led to the deliberate addition of rennet. Early archeological evidence of Egyptian cheese has been found in Egyptian tomb murals, dating to about 2000 BCE.[10] A 2018 scientific paper stated that the world's oldest cheese, dating to approximately 1200 BCE (3200 years before present), was found in ancient Egyptian tombs.[11][12]\n",
    "\n",
    "The earliest cheeses were likely quite sour and salty, similar in texture to rustic cottage cheese or feta, a crumbly, flavorful Greek cheese. Cheese produced in Europe, where climates are cooler than the Middle East, required less salt for preservation. With less salt and acidity, the cheese became a suitable environment for useful microbes and molds, giving aged cheeses their respective flavors. The earliest ever discovered preserved cheese was found in the Taklamakan Desert in Xinjiang, China, dating back as early as 1615 BCE (3600 years before present).\n",
    "\"\"\"\n",
    "\n",
    "run_model(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Evaluatie van model toevoegen(BLEU,ROUGE,METEOR,QA model)\n",
    "* Inference op GF\n",
    "* Op andere dataset trainen\n",
    "* Msschn andere params gebruiken"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
