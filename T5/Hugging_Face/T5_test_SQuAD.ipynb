{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "`pip install pytorch transformers datasets ntlk`\n",
    "## Source\n",
    "[source](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import list_datasets, load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "prefix = \"context: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('datasets.txt','w') as f:\n",
    "#     [f.write(d+'\\n') for d in list_datasets()]\n",
    "\n",
    "# with open('datasets.txt','r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# t5_qg_datasets = [line for line in lines if 't5' in line.lower() and ('qg' in line.lower() or 'question_generation' in line.lower())]\n",
    "# t5_qg_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/ManuV/.cache/huggingface/datasets/wiselinjayajos___parquet/wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 326.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_squad = load_dataset('wiselinjayajos/squad_modified_for_t5_qg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context', 'questions'],\n",
       "        num_rows: 18896\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['context', 'questions'],\n",
       "        num_rows: 2067\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'generate questions: Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'questions': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? {sep_token} What is in front of the Notre Dame Main Building? {sep_token} The Basilica of the Sacred heart at Notre Dame is beside to which structure? {sep_token} What is the Grotto at Notre Dame? {sep_token} What sits on top of the Main Building at Notre Dame? {sep_token}'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dict with keys ['context','questions']\n",
    "# 'context': 'generate questions: The Louvre ...'\n",
    "# 'questions': 'Where ... ? {sep_token} How ... ? {sep_token} ...? {sep_token}'\n",
    "dataset_squad[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\t5_test\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"context\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"questions\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\t5_test\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3586: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2625, 10, 3806, 746, 10, 30797, 120, 6, 8, 496, 65, 3, 9, 6502, 1848, 5, 71, 2916, 8, 5140, 5450, 31, 7, 2045, 22161, 19, 3, 9, 7069, 12647, 13, 8, 16823, 3790, 5, 3, 29167, 16, 851, 13, 8, 5140, 5450, 11, 5008, 34, 6, 19, 3, 9, 8658, 12647, 13, 2144, 28, 6026, 3, 76, 24266, 28, 8, 9503, 96, 553, 15, 7980, 1980, 1212, 13285, 1496, 1280, 3021, 12, 8, 5140, 5450, 19, 8, 23711, 2617, 13, 8, 3, 24756, 6219, 5, 3, 29167, 1187, 8, 20605, 2617, 19, 8, 8554, 17, 235, 6, 3, 9, 17535, 286, 13, 7029, 11, 9619, 5, 94, 19, 3, 9, 16455, 13, 8, 3, 3844, 17, 235, 44, 301, 1211, 1395, 6, 1410, 213, 8, 16823, 3790, 3, 28285, 26, 120, 4283, 12, 2788, 8942, 9, 26, 1954, 264, 8371, 8283, 16, 507, 3449, 5, 486, 8, 414, 13, 8, 711, 1262, 41, 232, 16, 3, 9, 1223, 689, 24, 1979, 7, 190, 220, 12647, 7, 11, 8, 2540, 10576, 15, 201, 19, 3, 9, 650, 6, 941, 3372, 12647, 13, 3790, 5, 1], [2625, 10, 3806, 746, 10, 282, 44, 167, 119, 8278, 6, 7711, 3, 17084, 31, 7, 481, 661, 3, 9, 381, 13, 1506, 783, 14290, 5, 37, 4169, 1236, 18, 4312, 14290, 560, 386, 16265, 6, 321, 3, 9, 2252, 11, 4390, 2478, 6, 11, 633, 13254, 11, 18178, 5, 10129, 202, 38, 3, 9, 80, 18, 6492, 6378, 16, 1600, 507, 3959, 6, 8, 16064, 40, 10057, 3835, 19, 4683, 4394, 3718, 11, 3213, 12, 36, 8, 10043, 7558, 3, 31003, 5707, 16, 8, 907, 1323, 5, 37, 119, 3835, 6, 37, 3736, 122, 12683, 6, 19, 1883, 4394, 3, 9, 215, 11, 3, 6915, 30, 1236, 6678, 11, 7924, 5, 37, 10576, 15, 215, 2567, 19, 1790, 10943, 5, 37, 16265, 43, 3, 14177, 5707, 3984, 6, 28, 37, 3, 16018, 49, 1790, 1444, 11, 3, 4894, 5099, 3819, 11, 119, 1506, 6, 11, 3, 30495, 57, 481, 45, 321, 7711, 3, 17084, 11, 2788, 3790, 31, 7, 1888, 5, 3, 8739, 16064, 40, 10057, 11, 37, 10576, 15, 6, 37, 3, 16018, 49, 19, 46, 2547, 5707, 11, 405, 59, 43, 3, 9, 6040, 8815, 42, 136, 14829, 21913, 45, 8, 636, 5, 86, 12701, 6, 116, 128, 481, 6141, 24, 37, 3, 16018, 49, 1553, 12, 504, 3, 9, 11252, 14387, 6, 3, 9, 10215, 8468, 6, 7155, 3, 19003, 47, 1790, 5, 3, 21322, 6, 16, 3888, 6, 116, 119, 481, 6141, 24, 8, 1040, 3217, 3, 9, 10215, 14387, 6, 8, 11252, 1040, 7262, 21608, 877, 139, 999, 5, 3, 19685, 1040, 19, 1790, 38, 557, 38, 37, 3, 16018, 49, 117, 983, 6, 66, 386, 33, 8308, 12, 66, 481, 5, 4213, 6, 16, 4328, 2628, 46, 12260, 6378, 21, 1827, 2056, 585, 6, 13594, 14984, 7, 6, 263, 165, 5695, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[304, 4068, 410, 8, 16823, 3790, 3, 18280, 2385, 16, 507, 3449, 16, 301, 1211, 1395, 1410, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 363, 19, 16, 851, 13, 8, 7711, 3, 17084, 5140, 5450, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 37, 23711, 2617, 13, 8, 3, 24756, 842, 44, 7711, 3, 17084, 19, 14898, 12, 84, 1809, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 363, 19, 8, 8554, 17, 235, 44, 7711, 3, 17084, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 363, 2561, 7, 30, 420, 13, 8, 5140, 5450, 44, 7711, 3, 17084, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 1], [366, 410, 8, 16064, 40, 10057, 8336, 13, 7711, 10157, 15, 1731, 9002, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 571, 557, 19, 7711, 3, 17084, 31, 7, 8, 3736, 122, 12683, 1790, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 363, 19, 8, 1444, 1236, 1040, 44, 7711, 3, 17084, 718, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 571, 186, 1236, 1506, 5778, 33, 435, 44, 7711, 3, 17084, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 86, 125, 215, 410, 8, 1236, 1040, 7155, 3, 19003, 1731, 5707, 44, 7711, 3, 17084, 58, 3, 2, 7, 15, 102, 834, 235, 2217, 2, 1]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(dataset_squad['train'][:2])# dict with keys ['input_ids','attention_mask','labels'] met inputs_ids=context en labels=questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\ManuV\\.cache\\huggingface\\datasets\\wiselinjayajos___parquet\\wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-76d040fbf063f08b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\ManuV\\.cache\\huggingface\\datasets\\wiselinjayajos___parquet\\wiselinjayajos--squad_modified_for_t5_qg-a090cde2e8fceb0a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-0043e0412c06186c.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = dataset_squad.map(preprocess, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,# Change to True if you train on GPU\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # Removing the sep_tokens so metric can be properly used\n",
    "    decoded_preds = [decoded_pred.split('\\n')[0] for decoded_pred in decoded_preds]\n",
    "    decoded_labels = [decoded_label.split('\\nsep_token') for decoded_label in decoded_labels]\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_scores = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        bleu_score = sentence_bleu([label], pred)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    result = {\"bleu\": np.mean(bleu_scores)}\n",
    "    # result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    # return {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=datasets.Dataset.from_dict(tokenized_squad[\"train\"][:50]),\n",
    "    eval_dataset=datasets.Dataset.from_dict(tokenized_squad[\"validation\"][:30]),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on CPU\n"
     ]
    }
   ],
   "source": [
    "if model.device.type == 'cuda':\n",
    "    print('Model is on GPU')\n",
    "\n",
    "    # Set model to train on CPU\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    trainer.device = device\n",
    "else:\n",
    "    print('Model is on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                             \n",
      "100%|██████████| 4/4 [00:28<00:00,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.268968105316162, 'eval_bleu': 0.0, 'eval_runtime': 10.6552, 'eval_samples_per_second': 2.816, 'eval_steps_per_second': 0.188, 'epoch': 1.0}\n",
      "{'train_runtime': 28.0694, 'train_samples_per_second': 1.781, 'train_steps_per_second': 0.143, 'train_loss': 4.160477638244629, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=4.160477638244629, metrics={'train_runtime': 28.0694, 'train_samples_per_second': 1.781, 'train_steps_per_second': 0.143, 'train_loss': 4.160477638244629, 'epoch': 1.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hij kan over 3 boomstammen springen.\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on some input text\n",
    "input_text = \"generate question: Napoleon is een generaal van Frankrijk. Hij heeft 4 prijzen gewonnen in Duitsland. Hij kan over 3 boomstammen springen.\"\n",
    "input_text = torch.tensor(tokenizer(input_text)['input_ids']).long().unsqueeze(0)\n",
    "generated = model.generate(input_text, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated output\n",
    "output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.268968105316162, 'eval_bleu': 0.0, 'eval_runtime': 8.4671, 'eval_samples_per_second': 3.543, 'eval_steps_per_second': 0.236, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(eval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Get the training and validation loss from the trainer\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(trainer\u001b[39m.\u001b[39;49mlog_metrics(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,metrics\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbleu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      5\u001b[0m train_loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlog_history[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m val_loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlog_history[\u001b[39m\"\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\t5_test\\lib\\site-packages\\transformers\\trainer_pt_utils.py:977\u001b[0m, in \u001b[0;36mlog_metrics\u001b[1;34m(self, split, metrics)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m***** \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m metrics *****\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 977\u001b[0m metrics_formatted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetrics_format(metrics)\n\u001b[0;32m    978\u001b[0m k_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mstr\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m metrics_formatted\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m    979\u001b[0m v_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mstr\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m metrics_formatted\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\ManuV\\AppData\\Local\\miniconda3\\envs\\t5_test\\lib\\site-packages\\transformers\\trainer_pt_utils.py:880\u001b[0m, in \u001b[0;36mmetrics_format\u001b[1;34m(self, metrics)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmetrics_format\u001b[39m(\u001b[39mself\u001b[39m, metrics: Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[0;32m    869\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[39m    Reformat Trainer metrics values to a human-readable format\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[39m        metrics (`Dict[str, float]`): The reformatted metrics\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m     metrics_copy \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    881\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m metrics_copy\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    882\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_mem_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m k:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training and validation loss from the trainer\n",
    "print(trainer.log_metrics(\"train\",metrics=\"bleu\"))\n",
    "train_loss = trainer.state.log_history[\"loss\"]\n",
    "val_loss = trainer.state.log_history[\"eval_loss\"]\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
