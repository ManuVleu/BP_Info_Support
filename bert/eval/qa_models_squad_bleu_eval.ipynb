{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "# All QA model names that need to be evaluated\n",
    "models = [\n",
    "    [\"damapika/roberta-base_mod\",386],\n",
    "    [\"damapika/distilbert-base-uncased_mod\",384],\n",
    "    [\"damapika/electra-base-discriminator_squad_mod\",386]\n",
    "]\n",
    "# Load the SQuAD validation dataset \n",
    "val_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "results=[]\n",
    "\n",
    "smoother = SmoothingFunction().method1\n",
    "for model_name in models:\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name[0])\n",
    "  model=AutoModelForQuestionAnswering.from_pretrained(model_name[0])\n",
    "  result=[]\n",
    "  bleu_scores=[]\n",
    "  for example in val_dataset:\n",
    "    \n",
    "    # Get the reference answer and question\n",
    "    reference_answer = example[\"answers\"][\"text\"][0]\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\",max_length=model_name[1],truncation=True)\n",
    "\n",
    "    # Generate an answer using the model\n",
    "    outputs = model(**inputs)\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "    predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index + 1].tolist()))\n",
    "    # Compute BLEU-4 score\n",
    "    reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer.lower())\n",
    "    bleu_score = sentence_bleu([reference_tokens], predicted_tokens, smoothing_function=smoother)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    print(bleu_score)\n",
    "        # Compute METEOR score\n",
    "        # meteor_score = meteor_score([reference_answer], predicted_answer)\n",
    "        # meteor_scores.append(meteor_score)\n",
    "\n",
    "    # Calculate the average scores\n",
    "  avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "  result.append(model_name[0])\n",
    "  result.append(avg_bleu_score)\n",
    "  results.append(result)\n",
    "  # Print the average evaluation results\n",
    "  print(model_name[0])\n",
    "  print(f\"Average BLEU-4 score: {avg_bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv('qa_models_squad_bleu_eval.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
