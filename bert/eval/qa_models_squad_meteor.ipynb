{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import meteor_score as calc_meteor\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/dama_/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m reference_tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(reference_answer\u001b[39m.\u001b[39mlower())\n\u001b[0;32m     32\u001b[0m predicted_tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(predicted_answer\u001b[39m.\u001b[39mlower())\n\u001b[1;32m---> 33\u001b[0m meteor_score \u001b[39m=\u001b[39m meteor_score([reference_tokens], predicted_tokens)\n\u001b[0;32m     34\u001b[0m meteor_scores\u001b[39m.\u001b[39mappend(meteor_score)\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(meteor_score)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    [\"damapika/roberta-base_mod\",386],\n",
    "    [\"damapika/distilbert-base-uncased_mod\",384],\n",
    "    [\"damapika/electra-base-discriminator_squad_mod\",386]\n",
    "]\n",
    "# Load the SQuAD validation dataset \n",
    "val_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "results=[]\n",
    "\n",
    "for model_name in models:\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name[0])\n",
    "  model=AutoModelForQuestionAnswering.from_pretrained(model_name[0])\n",
    "  result=[]\n",
    "  meteor_scores=[]\n",
    "  for example in val_dataset:\n",
    "    \n",
    "    # Get the reference answer and question\n",
    "    reference_answer = example[\"answers\"][\"text\"][0]\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\",max_length=model_name[1],truncation=True)\n",
    "\n",
    "    # Generate an answer using the model\n",
    "    outputs = model(**inputs)\n",
    "    start_index = torch.argmax(outputs.start_logits)\n",
    "    end_index = torch.argmax(outputs.end_logits)\n",
    "    predicted_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index + 1].tolist()))\n",
    "    # Compute METEOR score\n",
    "    reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer.lower())\n",
    "    meteor_score = calc_meteor([reference_tokens], predicted_tokens)\n",
    "    meteor_scores.append(meteor_score)\n",
    "    print(meteor_score)\n",
    "\n",
    "    # Calculate the average scores\n",
    "  avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "  result.append(model_name[0])\n",
    "  result.append(avg_meteor_score)\n",
    "  results.append(result)\n",
    "  # Print the average evaluation results\n",
    "  print(model_name[0])\n",
    "  print(f\"Average meteor score: {avg_meteor_score}\")\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('qa_models_squad_meteor_eval.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
