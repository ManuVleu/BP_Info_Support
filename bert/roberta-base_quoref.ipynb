{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset quoref (C:/Users/dama_/.cache/huggingface/datasets/quoref/default/0.1.0/82bb58a6b25cd8dbb4625a7ba6a5d0a224af1f4d392ca0de8b9e0c23e78557fe)\n",
      "100%|██████████| 2/2 [00:00<00:00, 31.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"quoref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'context', 'title', 'url', 'answers'],\n",
       "        num_rows: 19399\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'context', 'title', 'url', 'answers'],\n",
       "        num_rows: 2418\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "dataset_name = \"quoref\" \n",
    "model_type=\"roberta\"\n",
    "model_name= \"damapika/roberta-base_mod_squad\"\n",
    "models_dir = \"saved_models/roberta-base_mod_quoref\"\n",
    "checkpoint = 'roberta-base'\n",
    "max_input_length = 308\n",
    "\n",
    "\n",
    "# ## Training\n",
    "learning_rate = 3e-5\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max context length for dataset\n",
    "def calc_max_len(dataset):\n",
    "  context_length_max=len(dataset[0]['context'])\n",
    "  for i in range(len(dataset)):\n",
    "    con_len=len(dataset[i]['context'])\n",
    "    if(con_len<context_length_max):\n",
    "      context_length_max=con_len\n",
    "      print(context_length_max)\n",
    "      print(dataset[i]['context'])\n",
    "  return context_length_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n",
      "Set 19 years after the events of the first film, the movie deals with unresolved conflict and family strain, and also has elements of a coming of age story. Michael Goorjian reprises his role of Heroin Bob, and acts as a narrator for the film, both in voice over as well as sporadically intercut scenes of him in the afterlife\n",
      "Ross is the child of Trish and Heroin Bob, being conceived shortly before Bob's accidental drug overdose. Ross has been raised by alone by Trish, above her steam punk curio and clothing boutique, and as a result of his mothers adoration of the macabre, as well as his immersion in the concept of death from a young age, he develops into a Victorian Goth. Despite his obvious affiliation, Ross insists that he is part of no social cliques, which is stressed even further when he states that despite a lifelong abstinence from drugs, alcohol, and sex; he is not Straight Edge either. \n",
      "Upon having his heart broken by his first girlfriend, Ross attempts to drown his sorrows, as well as his lifelong espousal that romantic love is trivial, with the aide of beer and liquor. In an attempt to further help him take his mind off of things, he also begrudgingly goes on a road trip to a punk rock concert with his only friend, Crash, as well as Crash's friend Penny. Ross has a low opinion of punks, despite his association with them, as Crash and Penny are punk rockers, as was his father.\n",
      "1080\n",
      "The film begins with the bear family sleeping peacefully at home, when suddenly, the alarms of dozens of clocks located on Junyer Bear's table go off. Papa Bear wakes up completely and runs to try to turn them off. Junyer excitedly wakes up and exclaims: \"Oh, boy! At last the great day has come at last! Oh, boy!\" Papa Bear asks how to stop the alarms, and his son simply shushes the clocks. Dad gets angry and smacks a clock in Junyer's face. Mom replies: \"But, Henry ...\"  Henry shouts: \"Well! What do you Want!?\" To which Mom replies: \"It's Father's Day, Dear.\"\n",
      "Then Mama Bear and Junyer Bear make several activities to please Papa Bear on his day, but only cause discomfort and misery, ending with a theatrical presentation in which there are three numbers, of which the latter involves a song called, Let's Give a Cheer for Father. This number ends with Mama Bear and Junyer Bear dressed as parents of the American homeland (George Washington and Abraham Lincoln respectively), who disguise Papa Bear as the Statue of Liberty and shoot fireworks, as an allegory of July 4th.\n",
      "1066\n",
      "In Cleveland, Ohio, in 1978, four rebellious teenage boys - Hawk, Lex, Trip Verudie, and Jeremiah \"Jam\" Bruce - run their own Kiss tribute band called \"Mystery\" while they prepare to see their idols in concert in Detroit, Michigan, the following night. Their hopes, however, are dashed when Jam's religiously conservative mother discovers the concert tickets and burns them before having Jam transferred to a Catholic boarding school.\n",
      "Trip manages to win tickets and backstage passes from a radio contest in Detroit and the boys plan to rescue Jam from the boarding school. Disguised as pizza delivery boys, they get Father Phillip McNulty drugged from a pizza topped with hallucinogen mushrooms and set off with Jam for Detroit in Lex's mother's Volvo to pick up the tickets. While on the highway, they get into a road rage incident with disco fanatics Kenny and Bobby after Trip accidentally throws a slice of pizza on their windshield. They beat up the disco duo and continue their journey before picking up Christine, who walked out on Kenny due to his behavior.\n",
      "822\n",
      "Filmmaker Adam Green begins a documentary about artwork that features monsters.  Green is surprised when William Dekker, a retired police officer, contacts him and claims to have proof of the existence of monsters.  Green's wife reacts skeptically, but he reworks his documentary to focus on Dekker and his efforts to expose the monsters' underground home, which he calls \"The Marrow\". Green interviews Dekker at his house, who claims that he has seen many monsters and identified some of them through sketches. Dekker mentions his son once but diverts from the topic when Green inquires. The shooting crew of Green and his cameraman wait at the Marrow's entrance; a dug-up hole in the cemetery in the woods. On the first night they do not see anything although Dekker keeps claiming that he could see one of the monsters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "822"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_max_len(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ba3f052c7a557909526b59713430403dd134e01d',\n",
       " 'question': 'What is the first name of the person who doubted it would turn out to be a highly explosive eruption like those that can occur in subduction-zone volcanoes?',\n",
       " 'context': 'The earthquake swarm was noted on October 12, 2007 in the Prince George Citizen by citizen staff, three days after the earthquakes began. Scientists mentioned in the report were seismologist John Cassidy of Natural Resources Canada and volcanologist Catherine Hickson, who was part of the Geological Survey of Canada at the time. At the time of the report, scientists did not know the origin of the swarm. Seismologist John Cassidy stated, \"the depth is enough to rule out hydrothermal but it\\'s up in the air as to whether the cause is tectonic shifts or volcanic activity. If it is volcanic there are certain characteristics that we would expect, there\\'s a tremor-like character to it. And so we\\'ll be looking for the types of events that we see beneath volcanoes and we\\'ll be looking to see if they\\'re getting closer to the surface or if they\\'re migrating at all.\"Even if the Nazko swarm were a warning of a volcanic eruption, Hickson doubted it would turn out to be a highly explosive eruption like those that can occur in subduction-zone volcanoes. \"We\\'re not talking about an injection of tonnes of ash many kilometers into the air like the 1980 Mount St. Helens eruption or the 1991 Mount Pinatubo eruption. We\\'re talking about something very small, relatively localized that should have a fairly limited impact... but it\\'ll be extremely exciting\", Hickson said. If an eruption were to occur, Hickson suggested that it would be characterized by a lava fountain that sends globs of lava 100 m (330 ft) into the air. This is similar to those that occur in Hawaii. Hickson said that a Nazko eruption could be a tourist attraction, but warned that noxious gases such as carbon dioxide and sulfur dioxide would be released during the event.',\n",
       " 'title': '2007–2008 Nazko earthquakes 1',\n",
       " 'url': 'https://en.wikipedia.org/wiki/2007%E2%80%932008_Nazko_earthquakes',\n",
       " 'answers': {'answer_start': [250], 'text': ['Catherine']}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_input_length ,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\dama_\\.cache\\huggingface\\datasets\\quoref\\default\\0.1.0\\82bb58a6b25cd8dbb4625a7ba6a5d0a224af1f4d392ca0de8b9e0c23e78557fe\\cache-97bdbc22183b6cde.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to CUDA\n",
    "    device = torch.device('cuda')\n",
    "    print('gpu')\n",
    "else:\n",
    "    # If CUDA is not available, fall back to CPU\n",
    "    device = torch.device('cpu')\n",
    "    print('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/damapika/roberta-base_mod_quoref into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=models_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\dama_/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dama_\\OneDrive\\Desktop\\HoGent\\2022_2023\\BP\\BP_Info_Support\\bert\\wandb\\run-20230523_212946-zs93gfhm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damapika/BP_Info_Support/runs/zs93gfhm' target=\"_blank\">helpful-thunder-1</a></strong> to <a href='https://wandb.ai/damapika/BP_Info_Support' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damapika/BP_Info_Support' target=\"_blank\">https://wandb.ai/damapika/BP_Info_Support</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damapika/BP_Info_Support/runs/zs93gfhm' target=\"_blank\">https://wandb.ai/damapika/BP_Info_Support/runs/zs93gfhm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/damapika/BP_Info_Support/runs/zs93gfhm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x218977a1cd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 500/3639 [08:19<52:16,  1.00it/s]\n",
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1823, 'learning_rate': 2.1755976916735367e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1263, 'learning_rate': 1.763396537510305e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.266489028930664, 'eval_runtime': 22.7465, 'eval_samples_per_second': 106.302, 'eval_steps_per_second': 6.682, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8973, 'learning_rate': 1.3511953833470735e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7404, 'learning_rate': 9.389942291838417e-06, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3566540479660034, 'eval_runtime': 23.1232, 'eval_samples_per_second': 104.57, 'eval_steps_per_second': 6.573, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.687, 'learning_rate': 5.267930750206101e-06, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4962, 'learning_rate': 1.145919208573784e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5172, 'learning_rate': 0.0, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "                                                   \n",
      "100%|██████████| 3639/3639 [30:58<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5566297769546509, 'eval_runtime': 22.9847, 'eval_samples_per_second': 105.2, 'eval_steps_per_second': 6.613, 'epoch': 3.0}\n",
      "{'train_runtime': 1858.679, 'train_samples_per_second': 31.311, 'train_steps_per_second': 1.958, 'train_loss': 0.7955441685881252, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3639, training_loss=0.7955441685881252, metrics={'train_runtime': 1858.679, 'train_samples_per_second': 31.311, 'train_steps_per_second': 1.958, 'train_loss': 0.7955441685881252, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "To https://huggingface.co/damapika/roberta-base_mod_quoref\n",
      "   9d91ef2..ec50c72  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Fyodor Dostoevsky?\"\n",
    "context = \"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = transformers.pipeline(\"question-answering\", model=\"damapika/roberta-base_mod\")\n",
    "question_answerer(question=question, context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
