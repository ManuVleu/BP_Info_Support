{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset quoref (C:/Users/dama_/.cache/huggingface/datasets/quoref/default/0.1.0/82bb58a6b25cd8dbb4625a7ba6a5d0a224af1f4d392ca0de8b9e0c23e78557fe)\n",
      "100%|██████████| 2/2 [00:00<00:00, 334.73it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"quoref\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteredDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.filtered_indices = self._get_filtered_indices()\n",
    "\n",
    "    def _get_filtered_indices(self):\n",
    "        filtered_indices = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            item = self.dataset[i]\n",
    "            if len(item['answers']['answer_start']) > 0:\n",
    "                filtered_indices.append(i)\n",
    "        return filtered_indices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        original_index = self.filtered_indices[index]\n",
    "        return self.dataset[original_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# sep_token = '<sep>'\n",
    "dataset_name = \"adversarial_qa\"\n",
    "model_type=\"roberta\"\n",
    "model_name= \"roberta-base\"\n",
    "models_dir = \"saved_models/roberta-base_mod_quoref\"\n",
    "checkpoint = 'roberta-base'\n",
    "max_input_length = 308\n",
    "\n",
    "\n",
    "# ## Training\n",
    "learning_rate = 3e-5\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max context length for dataset\n",
    "def calc_max_len(dataset):\n",
    "  context_length_max=len(dataset[0]['context'])\n",
    "  for i in range(len(dataset)):\n",
    "    con_len=len(dataset[i]['context'])\n",
    "    if(con_len<context_length_max):\n",
    "      context_length_max=con_len\n",
    "      print(context_length_max)\n",
    "      print(dataset[i]['context'])\n",
    "  return context_length_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n",
      "In 1919, the Chicago White Sox are considered one of the greatest baseball teams ever assembled; however, the team's stingy owner, Charles Comiskey, gives little inclination to reward his players for a spectacular season.\n",
      "Gamblers \"Sleepy\" Bill Burns and Billy Maharg get wind of the players' discontent, asking shady player Chick Gandil to convince a select group of Sox—including star knuckleball pitcher Eddie Cicotte, who led the majors with a 29–7 win–loss record and an earned run average of 1.82—that they could earn more money by playing badly and throwing the series than they could earn by winning the World Series against the Cincinnati Reds . Cicotte was motivated because Comiskey refused him a promised $10,000 should he win 30 games for the season. Cicotte was nearing the milestone until Comiskey ordered team manager Kid Gleason to bench him for 2 weeks (missing 5 starts) with the excuse that the 35-year-old veteran's arm needed a rest before the series.\n",
      "A number of players, including Gandil, Swede Risberg, and Lefty Williams, go along with the scheme. Shoeless Joe Jackson, an illiterate and the team hitting star is also invited, but is depicted as being not bright and not entirely sure of what is going on. Buck Weaver, meanwhile, insists that he is a winner and wants nothing to do with the fix.\n",
      "1013\n",
      "Jim Madden, a Texas Ranger, is gunned down while investigating the murder of a local rancher.  His younger brother, Larry, vows to track down the suspected killer, another rancher named Joan Stanton.  While looking into the murders, he stumbles on a battle between Stanton, and a group of men working for another rancher, Frank Sanderson. Rescuing Stanton from the altercation, he keeps his identity as a Ranger secret, while attempting to learn the truth of what is going on.  Through talks with Stanton, Madden learns that Sanderson has been setting her up for both the murder of the other rancher, and Jim's death.\n",
      "Convinced by Stanton's story, Madden tells Stanton she must turn herself in, and she agrees. Before they can reach the Rangers, they are captured by Sanderson's men.  Sanderson plans to kill Madden, and take Stanton to Mexico.  With the help of Rangers' cook, Rusty, as well as several of Stanton's men, Madden overcomes Sanderson and his men, and takes a vindicated Stanton back to the Rangers.\n",
      "559\n",
      "Following an establishing shot of the New York City skyline, an elevator in a busy office building opens and happy-go-lucky Sky Ames steps out. In a joyful mood, singing to himself, he takes out a ring, puts it on third finger of his left hand and goes to the door marked \"Eaton, Eiton, Piper & Holland Advertising Agency\". Inside, Miss Wilson, secretary to his best friend, Jeff Holland tells him that Jeff is in a meeting. Showing her the ring, Sky explains that during the first vacation he took without Jeff, he met \"the most wonderful girl in the world\".\n",
      "451\n",
      "The story starts in sequence with 16-year-old Alison Findlay and her two friends playing a seemingly innocent ouija board game. Upon contacting a spirit, who is later revealed to be Alison's dead father (she never knew her actual parents), the girls discover that Alison is in danger. The spirit then possesses one of the girls and warns her not to return home for her 19th birthday. The girl is immediately killed after a bookcase collapses onto her.\n",
      "308\n",
      "Jack Murphy, a hardened, antisocial LAPD detective, frequently escapes the harsh reality that his ex-wife has become a stripper and his career is going nowhere by drinking. His world is turned upside down, however, when he is framed by ex-convict Joan Freeman for putting her in prison earlier in his career.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_max_len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ba3f052c7a557909526b59713430403dd134e01d',\n",
       " 'question': 'What is the first name of the person who doubted it would turn out to be a highly explosive eruption like those that can occur in subduction-zone volcanoes?',\n",
       " 'context': 'The earthquake swarm was noted on October 12, 2007 in the Prince George Citizen by citizen staff, three days after the earthquakes began. Scientists mentioned in the report were seismologist John Cassidy of Natural Resources Canada and volcanologist Catherine Hickson, who was part of the Geological Survey of Canada at the time. At the time of the report, scientists did not know the origin of the swarm. Seismologist John Cassidy stated, \"the depth is enough to rule out hydrothermal but it\\'s up in the air as to whether the cause is tectonic shifts or volcanic activity. If it is volcanic there are certain characteristics that we would expect, there\\'s a tremor-like character to it. And so we\\'ll be looking for the types of events that we see beneath volcanoes and we\\'ll be looking to see if they\\'re getting closer to the surface or if they\\'re migrating at all.\"Even if the Nazko swarm were a warning of a volcanic eruption, Hickson doubted it would turn out to be a highly explosive eruption like those that can occur in subduction-zone volcanoes. \"We\\'re not talking about an injection of tonnes of ash many kilometers into the air like the 1980 Mount St. Helens eruption or the 1991 Mount Pinatubo eruption. We\\'re talking about something very small, relatively localized that should have a fairly limited impact... but it\\'ll be extremely exciting\", Hickson said. If an eruption were to occur, Hickson suggested that it would be characterized by a lava fountain that sends globs of lava 100 m (330 ft) into the air. This is similar to those that occur in Hawaii. Hickson said that a Nazko eruption could be a tourist attraction, but warned that noxious gases such as carbon dioxide and sulfur dioxide would be released during the event.',\n",
       " 'title': '2007–2008 Nazko earthquakes 1',\n",
       " 'url': 'https://en.wikipedia.org/wiki/2007%E2%80%932008_Nazko_earthquakes',\n",
       " 'answers': {'answer_start': [250], 'text': ['Catherine']}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_input_length ,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\dama_\\.cache\\huggingface\\datasets\\quoref\\default\\0.1.0\\82bb58a6b25cd8dbb4625a7ba6a5d0a224af1f4d392ca0de8b9e0c23e78557fe\\cache-9b64e73348f9143b.arrow\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to CUDA\n",
    "    device = torch.device('cuda')\n",
    "    print('gpu')\n",
    "else:\n",
    "    # If CUDA is not available, fall back to CPU\n",
    "    device = torch.device('cpu')\n",
    "    print('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/damapika/roberta-base_mod into local empty directory.\n",
      "Download file pytorch_model.bin:   0%|          | 24.5k/473M [00:01<8:10:10, 16.9kB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download file pytorch_model.bin:  99%|█████████▉| 468M/473M [00:20<00:00, 26.2MB/s]   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download file pytorch_model.bin: 100%|██████████| 473M/473M [00:30<00:00, 26.2MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download file pytorch_model.bin: 100%|██████████| 473M/473M [00:40<00:00, 12.4MB/s]\n",
      "Download file training_args.bin: 100%|██████████| 3.56k/3.56k [00:39<?, ?B/s]\n",
      "\n",
      "\u001b[A\n",
      "Download file runs/Apr22_12-18-54_Damapika/1682158740.0244384/events.out.tfevents.1682158740.Damapika.29852.1: 100%|██████████| 5.72k/5.72k [00:39<?, ?B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Download file runs/Apr22_12-18-54_Damapika/events.out.tfevents.1682158739.Damapika.29852.0: 100%|██████████| 15.1k/15.1k [00:39<?, ?B/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Clean file training_args.bin: 100%|██████████| 3.56k/3.56k [00:39<00:00, 66.9B/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Clean file runs/Apr22_12-18-54_Damapika/1682158740.0244384/events.out.tfevents.1682158740.Damapika.29852.1: 100%|██████████| 5.72k/5.72k [00:39<00:00, 124B/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Clean file runs/Apr22_12-18-54_Damapika/events.out.tfevents.1682158739.Damapika.29852.0: 100%|██████████| 15.1k/15.1k [00:39<00:00, 370B/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Clean file pytorch_model.bin: 100%|██████████| 473M/473M [00:20<00:00, 24.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=models_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dama_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamapika\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\HOGENT\\2022_2023\\BA\\BP_Info_Support\\bert\\wandb\\run-20230518_115002-hsxyfzwy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damapika/huggingface/runs/hsxyfzwy' target=\"_blank\">kind-disco-19</a></strong> to <a href='https://wandb.ai/damapika/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damapika/huggingface' target=\"_blank\">https://wandb.ai/damapika/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damapika/huggingface/runs/hsxyfzwy' target=\"_blank\">https://wandb.ai/damapika/huggingface/runs/hsxyfzwy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 500/3639 [16:03<1:17:02,  1.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3474, 'learning_rate': 2.5877988458367684e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1000/3639 [29:37<1:05:27,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6272, 'learning_rate': 2.1755976916735367e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1213/3639 [35:27<50:08,  1.24s/it]  \n",
      " 33%|███▎      | 1213/3639 [37:05<50:08,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4654324054718018, 'eval_runtime': 98.187, 'eval_samples_per_second': 24.626, 'eval_steps_per_second': 1.548, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1500/3639 [45:11<56:59,  1.60s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2536, 'learning_rate': 1.763396537510305e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 2000/3639 [1:00:21<59:34,  2.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0583, 'learning_rate': 1.3511953833470735e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2426/3639 [1:28:11<27:00,  1.34s/it]    \n",
      " 67%|██████▋   | 2426/3639 [1:29:38<27:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4134212732315063, 'eval_runtime': 87.0611, 'eval_samples_per_second': 27.774, 'eval_steps_per_second': 1.746, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 2500/3639 [1:31:42<30:34,  1.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9832, 'learning_rate': 9.389942291838417e-06, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 3000/3639 [1:46:13<17:00,  1.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7188, 'learning_rate': 5.267930750206101e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 3500/3639 [2:00:05<03:42,  1.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6854, 'learning_rate': 1.145919208573784e-06, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3639/3639 [2:04:13<00:00,  1.35s/it]\n",
      "100%|██████████| 3639/3639 [2:05:28<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5400279760360718, 'eval_runtime': 75.6131, 'eval_samples_per_second': 31.979, 'eval_steps_per_second': 2.01, 'epoch': 3.0}\n",
      "{'train_runtime': 7531.7793, 'train_samples_per_second': 7.727, 'train_steps_per_second': 0.483, 'train_loss': 1.2186447462232337, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3639, training_loss=1.2186447462232337, metrics={'train_runtime': 7531.7793, 'train_samples_per_second': 7.727, 'train_steps_per_second': 0.483, 'train_loss': 1.2186447462232337, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload file pytorch_model.bin: 488MB [00:45, 13.5MB/s]                            To https://huggingface.co/damapika/roberta-base_mod\n",
      "   498b557..185f8b2  main -> main\n",
      "\n",
      "Upload file pytorch_model.bin: 100%|██████████| 473M/473M [00:46<00:00, 10.8MB/s]\n",
      "Upload file runs/May18_11-49-05_Damapika/events.out.tfevents.1684403400.Damapika.30672.0: 100%|██████████| 6.18k/6.18k [00:46<00:00, 137B/s] \n",
      "To https://huggingface.co/damapika/roberta-base_mod\n",
      "   185f8b2..ba9f726  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/damapika/roberta-base_mod/commit/185f8b2a6bf3c574dd723f8ea4303663196f482f'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Fyodor Dostoevsky?\"\n",
    "context = \"In the world of literature, there have been many authors who have gained a reputation for their ability to create complex characters. One such author is Fyodor Dostoevsky, a Russian novelist who wrote several influential works in the 19th century.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = transformers.pipeline(\"question-answering\", model=\"damapika/roberta-base_mod\")\n",
    "question_answerer(question=question, context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
